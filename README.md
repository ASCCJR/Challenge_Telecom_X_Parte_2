# An√°lise Preditiva de Evas√£o de Clientes (Churn) em Telecom

## üìñ Sobre o Projeto
Este reposit√≥rio cont√©m o projeto desenvolvido como parte do programa de forma√ß√£o em Data Science oferecido pela Oracle em parceria com a Alura. O objetivo principal √© realizar uma an√°lise completa sobre a evas√£o (churn) de clientes em uma empresa de telecomunica√ß√µes, desenvolvendo um modelo de Machine Learning capaz de prever quais clientes t√™m maior probabilidade de cancelar seus servi√ßos.

Al√©m da modelagem preditiva, o projeto foca em identificar os fatores mais influentes na decis√£o de churn, traduzindo os resultados t√©cnicos em insights acion√°veis e estrat√©gias de neg√≥cio para a reten√ß√£o de clientes.

## üõ†Ô∏è Tecnologias e Bibliotecas
O projeto foi desenvolvido em Python no ambiente Google Colab e utiliza as seguintes bibliotecas principais:

- **pandas**: Para manipula√ß√£o e an√°lise de dados estruturados.
- **scikit-learn**: Para pr√©-processamento de dados, modelagem e avalia√ß√£o de m√©tricas.
- **imblearn**: Para aplicar t√©cnicas de balanceamento de dados, como o SMOTE.
- **xgboost**: Para treinar o modelo Gradient Boosting e extrair a import√¢ncia das vari√°veis.
- **matplotlib & seaborn**: Para a cria√ß√£o de visualiza√ß√µes gr√°ficas e estat√≠sticas.
- **requests & json**: Para extrair e manipular os dados brutos no formato JSON.

## üî¨ Metodologia de Modelagem
A abordagem para a constru√ß√£o do modelo preditivo foi sistem√°tica e comparativa, garantindo a escolha da solu√ß√£o mais eficaz para o problema de neg√≥cio.

### 1. Desafio: Dados Desbalanceados
A an√°lise explorat√≥ria inicial revelou um forte desbalanceamento na vari√°vel alvo, com uma propor√ß√£o muito maior de clientes n√£o-evasivos. Para mitigar o risco de um modelo enviesado e com baixa capacidade de prever a classe minorit√°ria (Churn), foi utilizada a t√©cnica de oversampling SMOTE (Synthetic Minority Over-sampling Technique).

### 2. Modelos Avaliados
Foram treinados e avaliados tr√™s algoritmos de classifica√ß√£o distintos, tanto nos dados originais quanto nos dados balanceados pelo SMOTE, para uma compara√ß√£o robusta:

- **Regress√£o Log√≠stica**: Um modelo linear robusto e altamente interpret√°vel.
- **Random Forest**: Um modelo baseado em √°rvores de decis√£o, eficaz para capturar intera√ß√µes complexas.
- **XGBoost**: Um modelo de Gradient Boosting conhecido por sua alta performance e precis√£o.

### 3. M√©trica de Sucesso
Dado o contexto de neg√≥cio, onde o custo de n√£o identificar um cliente que ir√° evadir (falso negativo) √© alto, a m√©trica priorit√°ria para avalia√ß√£o foi o **Recall**.

## üöÄ Desempenho e Resultados
Ap√≥s a fase de treinamento, os modelos foram avaliados no conjunto de teste. A tabela abaixo resume o desempenho comparativo:

### Comparativo de Desempenho

| Modelo                          | Recall | Precis√£o | F1-Score | ROC AUC |
|---------------------------------|--------|----------|----------|---------|
| Regress√£o Log√≠stica (Original)  | 0.55   | 0.65     | 0.60     | 0.84    |
| Random Forest (Original)        | 0.49   | 0.65     | 0.56     | 0.82    |
| XGBoost (Original)              | 0.52   | 0.68     | 0.59     | 0.84    |
| Regress√£o Log√≠stica (SMOTE)     | **0.79** | 0.51     | 0.62     | 0.84    |

### Conclus√£o da An√°lise de Modelos
Os modelos treinados nos dados originais apresentaram um Recall baixo, sendo incapazes de identificar cerca de metade dos clientes que de fato cancelaram o servi√ßo. A aplica√ß√£o da t√©cnica de balanceamento SMOTE no modelo de Regress√£o Log√≠stica resultou em um aumento dr√°stico do Recall para **79%**. Embora isso tenha diminu√≠do a Precis√£o, o ganho na capacidade de identificar clientes em risco justifica a troca, alinhando o desempenho do modelo ao objetivo principal do neg√≥cio.

A **Regress√£o Log√≠stica treinada com dados balanceados pelo SMOTE** foi, portanto, selecionada como a solu√ß√£o final.

## üìä Principais Gr√°ficos e Observa√ß√µes
Para ilustrar os resultados da an√°lise, os seguintes gr√°ficos extra√≠dos do notebook s√£o essenciais:

### 1. Desbalanceamento da Vari√°vel Alvo
Gr√°fico de barras mostrando a contagem de clientes "Churn: Sim" vs. "Churn: N√£o".  
![](IMAGEM_1.png)

**Observa√ß√£o**: O gr√°fico evidencia a necessidade de t√©cnicas de balanceamento de dados para garantir que o modelo aprenda a identificar corretamente a classe minorit√°ria (churn).

### 2. Import√¢ncia das Vari√°veis (Feature Importance)
Gr√°fico de barras horizontais mostrando as 10 vari√°veis mais importantes segundo o modelo XGBoost.  
![](IMAGEM_2.png) 

**Observa√ß√£o**: A an√°lise de import√¢ncia revela que o tipo de contrato mensal, o baixo tempo de perman√™ncia (tenure) e o servi√ßo de internet fibra √≥ptica s√£o os fatores mais decisivos para prever a evas√£o de um cliente. Esta informa√ß√£o √© crucial para direcionar as estrat√©gias de reten√ß√£o.

## üí° Estrat√©gias de Reten√ß√£o Propostas
Com base nos resultados, foram propostas quatro estrat√©gias principais:

1. **Fideliza√ß√£o atrav√©s de Contratos de Longo Prazo**: Incentivar a migra√ß√£o de clientes do plano mensal para planos anuais/bianuais.  
2. **Programa de Acolhimento para Novos Clientes**: Acompanhamento focado nos primeiros meses para garantir uma boa experi√™ncia.  
3. **Auditoria do Servi√ßo de Fibra √ìptica**: Investigar e corrigir poss√≠veis problemas de pre√ßo, qualidade ou suporte relacionados a este servi√ßo.  
4. **Venda de Valor Agregado**: Incentivar a contrata√ß√£o de servi√ßos adicionais de seguran√ßa e suporte t√©cnico para aumentar o engajamento.  
